{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With KNN, RAG, or Classification, we can recommend specific wines. See the diagram below for how we make this suggestion in each approach. \n",
    "\n",
    "In this notebook, we evaluate how well each modelling approach suggests top 5 wines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='misc/Evaluate Top 5 Wines.png' width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use LLM to generate the *Relevance* score to evaluate the wines reccommended by each model. We ask the AI to measure relevance of the result, which asseses the appropriateness and applicability of the wine reccommdations with respect to the user query. We test with 100 queries and report the mean relevance score for each model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfSB0NvY_8iu",
    "outputId": "00237bef-bfc5-49d7-96be-49926a8c2e55"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   transformers==4.31.0 \\\n",
    "#   pinecone-client==2.2.4 \\\n",
    "#   openai==1.3.2 \\\n",
    "#   tiktoken==0.5.1 \\\n",
    "#   langchain==0.0.336 \\\n",
    "#   lark==1.1.8 \\\n",
    "#   cohere==4.27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2btoEd8YBKD9"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjrTkEAxBjbN"
   },
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l-oun_CKAJYG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# get API key from OpenAI website\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the LLM Chain with prompt to assess Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo-1106', \n",
    "    # model_name='gpt-4-1106-preview', \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# In put the prompt to assess Relevance\n",
    "template_rel = \"\"\"\n",
    "\n",
    "As the 'Relevance Judge', your role involves evaluating the relevance of each QUERY for wine suggestion and a RESULT (which is a wine recommendation table), providing a score from 1 to 5.\n",
    "You will receive a tuple of (QUERY, RESULT), and must give an overall score for the tuple.\n",
    "\n",
    "score: Your numerical score for the model's relevance based on the rubric\n",
    "justification: Your step-by-step reasoning about the model's relevance score\n",
    "\n",
    "Please give a score from 1-5 based on the degree of relevance to the query, where the lowest and highest scores are defined as follows:\n",
    "Score 1: The result doesn't mention anything about the question or is completely irrelevant to the query.\n",
    "Score 5: The result addresses all aspects of the question and all parts of the result are meaningful and relevant to the question.\n",
    "\n",
    "You must format your answer in a well-formatted markdown table with only one single row, and columns for QUERY, RESULT, score, and justification.\n",
    "This format will help to clearly present your analysis.\n",
    "\n",
    "Your score should measures the appropriateness and applicability of the result with respect to the query.\n",
    "Scores should reflect the extent to which the result directly addresses the question provided in the query, and give lower scores for incomplete or redundant result.\n",
    "\n",
    "Maintain a formal and technical tone, focusing on impartial and objective analysis. Avoid irrelevant discussions and concentrate on the alignment between the recommendations and the query specifics.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "RESULT: {result}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_rel = PromptTemplate.from_template(template=template_rel)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to convert LLM output to Pandas DataFrame\n",
    "def get_df_for_result(res):\n",
    "    \"\"\"\n",
    "    Convert the the Markdown content to Pandas DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "    res_text = res['text']\n",
    "        \n",
    "    # Convert to pandas dataframe\n",
    "    rows = res_text.split('\\n')    \n",
    "    split_rows = [r.split('|') for r in rows]\n",
    "    \n",
    "    split_rows_clean=[]\n",
    "    for r in split_rows:\n",
    "        clean_row =  [c.strip() for c in r if c!='']\n",
    "        split_rows_clean.append(clean_row)\n",
    "    \n",
    "    # Extract the header and data rows\n",
    "    header = split_rows_clean[0]\n",
    "    data = split_rows_clean[2:]\n",
    "    \n",
    "    # Create a pandas DataFrame using the extracted header and data rows\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define helper function to extract the evaluation score from the LLM output\n",
    "def get_score_test(res):\n",
    "    \"\"\"\n",
    "    Get the evaluation score from the Mardown content.\n",
    "\n",
    "    \"\"\"\n",
    "    res_text = res['text']   \n",
    "    score_field = re.findall(\"\\|\\s*(\\d)\\s*\\|\", res['text'])\n",
    "\n",
    "    if len(score_field)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return int(score_field[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test this out with a few (query, result) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [{\"query\": \"What red wines would you suggest for someone who enjoys a velvety texture and soft tannins?\",\n",
    "              \"result\": \"\"\"\n",
    "                        | Title | Description | Variety | Country | Region | Winery | Province |\n",
    "                        | --- | --- | --- | --- | --- | --- | --- |\n",
    "                        | Jean-Luc and Paul Aegerter 2014 Vieilles Vignes  (Savigny-lÃ¨s-Beaune) | A smooth wine with a balance between red fruits and soft tannins. A smoky edge mingles with the berry-fruit acidity. | Pinot Noir | France | Savigny-lÃ¨s-Beaune | Jean-Luc and Paul Aegerter | Burgundy |\n",
    "                        | Martin Ray 2009 Synthesis Red (Napa Valley) | Very rich and smooth in texture, with lots of tannins that are soft, ripe and easy. Offers waves of blackberry, black currant, dark chocolate and caramelized oak flavors. | Bordeaux-style Red Blend | US | Napa Valley | Martin Ray | California |\n",
    "                        | Sogevinus 2006 D + D Red (Douro) | A Portuguese wine with minty, ripe, velvet textures, smooth and full-bodied. Very polished and classy, with soft acidity, tarry tannins, new wood and plum juice flavors. | Portuguese Red | Portugal | Douro | Sogevinus | Douro |\n",
    "                        | Tenuta di Ghizzano 2004 Veneroso Red (Toscana) | Modern, soft and velvety in texture, with hints of Amaretto, soy sauce and pipe tobacco. The mouthfeel is exceptional with bright fruit tones, chewy consistency and long-lasting menthol freshness. | Red Blend | Italy | Toscana | Tenuta di Ghizzano | Tuscany |\n",
    "                        | Luis Duarte 2014 Rubrica Tinto Red (Alentejano) | A generous, opulent wine with fine, velvet-smooth tannins, a background dryness that gives structure. Black fruits shine through, enhanced by the spice and toast of the wood aging. | Portuguese Red | Portugal | Alentejano | Luis Duarte | Alentejano |\n",
    "                        \n",
    "                        \"\"\"}, \n",
    "              {\"query\": \"Can you suggest a wine from Germany?\",\n",
    "              \"result\": \"\"\"\n",
    "                        | Title | Description | Variety | Country | Region | Winery | Province |\n",
    "                        | --- | --- | --- | --- | --- | --- | --- |\n",
    "                        | Fritz MÃ¼ller NV Perlwein Trocken MÃ¼ller-Thurgau (Rheinhessen) | A fun and refreshing lightly sparkling white wine with floral notes and a chalky, earthen tang. | MÃ¼ller-Thurgau | Germany | Rheinhessen | Fritz MÃ¼ller | Rheinhessen |\n",
    "                        | Grafen Neipperg 2008 Trocken SpÃ¤tburgunder (WÃ¼rttemberg) | A vibrant Pinot Noir with strawberry and raspberry fruit and slightly herbal nuances. | SpÃ¤tburgunder | Germany | WÃ¼rttemberg | Grafen Neipperg | WÃ¼rttemberg |\n",
    "                        | Wagner-Stempel 2014 Gutswein Trocken Weissburgunder (Rheinhessen) | A sprightly Weissburgunder with crisp white peach and apricot flavor. | Weissburgunder | Germany | Rheinhessen | Wagner-Stempel | Rheinhessen |\n",
    "                        | Flying Ace 2006 Red Red (Pfalz) | A red wine blend delivering black cherry fruit, medium body and soft tannins. | Red Blend | Germany | Pfalz | Flying Ace | Pfalz |\n",
    "                        | Fitz-Ritter 2006 Beerenauslese Rieslaner (Pfalz) | An intensely botrytized wine with dried apricot aromas and orange marmalade flavors. | Rieslaner | Germany | Pfalz | Fitz-Ritter | Pfalz |\n",
    "                        \n",
    "                        \"\"\"},\n",
    "              {\"query\": \"Can you recommend a red wine that's elegant and well-structured?\",\n",
    "              \"result\": \"\"\"\n",
    "                        | Title | Description | Variety | Country | Region | Winery | Province |\n",
    "                        | --- | --- | --- | --- | --- | --- | --- |\n",
    "                        | VilafontÃ© 2012 Series C Red (Paarl) | A blend of Cabernet Sauvignon, Malbec, Merlot and Cabernet Franc, with spicy aromas and a solid fruit core. Well balanced, with velvety tannins and a long, evolving finish. | Bordeaux-style Red Blend | South Africa | Paarl | VilafontÃ© | Paarl |\n",
    "                        | Tardieu-Laurent 2010 Vieilles Vignes  (Gigondas) | Starts off a bit stern in texture, but by the long finish those tannins have turned velvety and plush, nicely framing intense flavors. | RhÃ´ne-style Red Blend | France | Gigondas | Tardieu-Laurent | RhÃ´ne Valley |\n",
    "                        | VilafontÃ© 2005 Series C Red (Paarl) | An elegant, mouthwatering red blend with ripe tannins and a long finish. | Bordeaux-style Red Blend | South Africa | Paarl | VilafontÃ© | Paarl |\n",
    "                        | Duckhorn 2004 Red Wine Red (Howell Mountain) | Almost entirely Merlot, this Howell Mountain red has a deep core of blackberries and currants, with velvety tannins. | Bordeaux-style Red Blend | US | Howell Mountain | Duckhorn | California |\n",
    "                        | Ernie Els 2009 Signature Red (Stellenbosch) | This Bordeaux-style blend is oaky at first, but with a lush, mouthfilling palate and a texture like crushed velvet. | Bordeaux-style Red Blend | South Africa | Stellenbosch | Ernie Els | Stellenbosch |\n",
    "                                                \n",
    "                        \"\"\"},\n",
    "            {\"query\": \"Suggest a wine from a winery known for its award-winning practices.\",\n",
    "              \"result\": \"\"\"\n",
    "                        I'm sorry, but the context does not provide information about a winery known for its award-winning practices.                        \n",
    "                        \"\"\"}\n",
    "              ]\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_rel)\n",
    "score_tab = llm_chain.apply(input_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| QUERY | RESULT | score | justification |\n",
       "| --- | --- | --- | --- |\n",
       "| What red wines would you suggest for someone who enjoys a velvety texture and soft tannins? | The result provides a selection of red wines with descriptions that specifically mention velvety texture and soft tannins. Each wine is described in detail, highlighting the smoothness and softness of the tannins, which directly aligns with the query. The wines are from various countries and regions, offering a diverse range of options. Therefore, the result is highly relevant to the query and addresses all aspects of the question. | 5 | The result directly addresses the query by providing detailed descriptions of red wines with velvety texture and soft tannins, meeting all aspects of the question and offering a diverse selection of relevant recommendations. Therefore, it deserves a score of 5 for its high degree of relevance. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| QUERY | RESULT | score | justification |\n",
       "| --- | --- | --- | --- |\n",
       "| Can you suggest a wine from Germany? | The result provides a table of wine recommendations from Germany, including the title, description, variety, country, region, winery, and province of each wine. Each wine listed is from a different region in Germany, showcasing the diversity of German wines. The descriptions of each wine provide a good overview of the flavor profile and characteristics, allowing for informed decision-making. Therefore, the result is highly relevant to the query and addresses all aspects of the question. | 5 | The result directly addresses the query by providing a comprehensive list of wine recommendations from Germany, covering different varieties and regions. The descriptions of each wine also align with the query by offering detailed information about the suggested wines, making the result highly relevant and informative. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| QUERY | RESULT | score | justification |\n",
       "| --- | --- | --- | --- |\n",
       "| Can you recommend a red wine that's elegant and well-structured? | The result provides a list of red wines with detailed descriptions, including information about the variety, country, region, and winery. The wines are described as elegant, well-structured, and velvety, which aligns with the query. | 5 | The result directly addresses the query by providing a selection of red wines that are described as elegant and well-structured. The descriptions of the wines' characteristics and flavors demonstrate a high level of relevance to the query. Therefore, the score of 5 is justified based on the comprehensive and meaningful alignment between the recommendations and the query specifics. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| QUERY | RESULT | Score | Justification |\n",
       "|-------|--------|-------|----------------|\n",
       "| Suggest a wine from a winery known for its award-winning practices. | The result does not mention anything about a winery known for its award-winning practices. | 1 | The result is completely irrelevant to the query as it does not address the request for a wine suggestion from a winery known for its award-winning practices. Therefore, it receives a score of 1. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the evaluation results for these 4 cases\n",
    "display(Markdown(score_tab[0]['text']))\n",
    "\n",
    "display(Markdown(score_tab[1]['text']))\n",
    "\n",
    "display(Markdown(score_tab[2]['text']))\n",
    "\n",
    "display(Markdown(score_tab[3]['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work well. Now we evaluate with 100 test queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate RAG wine recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG test result dataset\n",
    "file_path = '../Data/rag_results.csv'\n",
    "wine_queries = pd.read_csv(file_path)\n",
    "\n",
    "# Get all 100 test cases in a list\n",
    "test_input = [{\"query\": r['question'], \"result\": f\"\"\"{r['result']}\"\"\"} for i, r in wine_queries.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|█████████████████████████████████████████████████████████████| 10/10 [17:07<00:00, 102.73s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Calculate the number of batches needed\n",
    "# num_batches = math.ceil(len(test_input[:20]) / batch_size)  # This process the full 20 cases in 2 batches of 10\n",
    "num_batches = math.ceil(len(test_input) / batch_size)  # This process the full 100 cases in 10 batches of 10\n",
    "\n",
    "# Initialize lists to store the eval results and scores\n",
    "eval_results = []\n",
    "\n",
    "# Process data in batches with tqdm\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(test_input))\n",
    "    batch = test_input[start_idx:end_idx]\n",
    "\n",
    "    # Score each tuple in the batch\n",
    "    batch_score = llm_chain.apply(batch) \n",
    "\n",
    "    # Extract the score out of the eval result and append to the score list\n",
    "    for i in batch_score:\n",
    "        eval_results.append(i)\n",
    "\n",
    "    time.sleep(8)\n",
    "\n",
    "# Store these lists for RAG\n",
    "eval_results_rag = eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scores from the evals and compute mean relevance score\n",
    "eval_scores_rag = []\n",
    "\n",
    "for i in eval_results_rag:\n",
    "    score_i = get_score_test(i)\n",
    "    eval_scores_rag.append(score_i)\n",
    "\n",
    "eval_scores_rag = [i for i in eval_scores_rag if i!=None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Revelance Score for RAG's wine recs: 4.84\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean Revelance score\n",
    "m_rel_rag = np.mean(eval_scores_rag)\n",
    "\n",
    "print('Mean Revelance Score for RAG\\'s wine recs:', m_rel_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate KNN wine recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG test result dataset\n",
    "file_path = '../Data/knn_results.csv'\n",
    "wine_queries = pd.read_csv(file_path)\n",
    "\n",
    "# Get all 100 test cases in a list\n",
    "test_input = [{\"query\": r['question'], \"result\": f\"\"\"{r['result']}\"\"\"} for i, r in wine_queries.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|███████████████████████████████████████████████████████████| 34/34 [3:15:27<00:00, 344.93s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 3\n",
    "\n",
    "# Calculate the number of batches needed\n",
    "# num_batches = math.ceil(len(test_input[:10]) / batch_size)  # This process the full 10 cases in 4 batches of 3\n",
    "num_batches = math.ceil(len(test_input) / batch_size)  # This process the full 100 cases in 34 batches of 3\n",
    "\n",
    "# Initialize lists to store the eval results and scores\n",
    "eval_results = []\n",
    "# eval_scores = []\n",
    "\n",
    "# Process data in batches with tqdm\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(test_input))\n",
    "    batch = test_input[start_idx:end_idx]\n",
    "\n",
    "    # Score each tuple in the batch\n",
    "    batch_score = llm_chain.apply(batch) \n",
    "\n",
    "    # Extract the score out of the eval result and append to the score list\n",
    "    for i in batch_score:\n",
    "        eval_results.append(i)\n",
    "        # score_i = get_score_test(i)\n",
    "        # eval_scores.append(score_i)\n",
    "\n",
    "    time.sleep(8)\n",
    "\n",
    "# Store these lists for KNN\n",
    "eval_results_knn = eval_results\n",
    "# eval_scores_knn = [i for i in eval_scores if i!=None]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores_knn = []\n",
    "\n",
    "for i in eval_results_knn:\n",
    "    score_i = get_score_test(i)\n",
    "    eval_scores_knn.append(score_i)\n",
    "\n",
    "eval_scores_knn = [i for i in eval_scores_knn if i!=None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Revelance Score for KNN's wine recs: 4.493333333333333\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean Revelance score\n",
    "m_rel_knn = np.mean(eval_scores_knn)\n",
    "\n",
    "print('Mean Revelance Score for KNN\\'s wine recs:', m_rel_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Classification wine recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG test result dataset\n",
    "file_path = '../Data/classification_results.csv'\n",
    "wine_queries = pd.read_csv(file_path)\n",
    "\n",
    "# Get all 100 test cases in a list\n",
    "test_input = [{\"query\": r['Question'], \"result\": f\"\"\"{r['Result']}\"\"\"} for i, r in wine_queries.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|███████████████████████████████████████████████████████████| 34/34 [4:07:34<00:00, 436.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 3\n",
    "\n",
    "# Calculate the number of batches needed\n",
    "# num_batches = math.ceil(len(test_input[:10]) / batch_size)  # This process the full 10 cases in 4 batches of 3\n",
    "num_batches = math.ceil(len(test_input) / batch_size)  # This process the full 100 cases in 34 batches of 3\n",
    "\n",
    "# Initialize lists to store the eval results and scores\n",
    "eval_results = []\n",
    "\n",
    "# Process data in batches with tqdm\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(test_input))\n",
    "    batch = test_input[start_idx:end_idx]\n",
    "\n",
    "    # Score each tuple in the batch\n",
    "    batch_score = llm_chain.apply(batch) \n",
    "\n",
    "    # Extract the score out of the eval result and append to the score list\n",
    "    for i in batch_score:\n",
    "        eval_results.append(i)\n",
    "\n",
    "\n",
    "    time.sleep(8)\n",
    "\n",
    "# Store these lists for KNN\n",
    "eval_results_clf = eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scores from the evals and compute mean relevance score\n",
    "eval_scores_clf = []\n",
    "\n",
    "for i in eval_results_clf:\n",
    "    score_i = get_score_test(i)\n",
    "    eval_scores_clf.append(score_i)\n",
    "\n",
    "eval_scores_clf = [i for i in eval_scores_clf if i!=None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Revelance Score for Classification's wine recs: 4.34375\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean Revelance score\n",
    "m_rel_clf = np.mean(eval_scores_clf)\n",
    "\n",
    "print('Mean Revelance Score for Classification\\'s wine recs:', m_rel_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision table across 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_rel_data = {'0': ['KNN Search', m_rel_knn],\n",
    "                '1': ['RAG', m_rel_rag],\n",
    "                '2': ['Classification', m_rel_clf] }\n",
    "\n",
    "# Make a datframe of final table\n",
    "tab_rel_final = pd.DataFrame.from_dict(tab_rel_data, \n",
    "                                           orient='index', \n",
    "                                           columns=['Model', 'Mean Relevance Score'])\n",
    "\n",
    "# Save to CSV file\n",
    "tab_rel_final.to_csv('results_to_eval/top_5_wines/compare_KNN_RAG_CLF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean Relevance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Search</td>\n",
       "      <td>4.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAG</td>\n",
       "      <td>4.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classification</td>\n",
       "      <td>4.343750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Mean Relevance Score\n",
       "0      KNN Search              4.493333\n",
       "1             RAG              4.840000\n",
       "2  Classification              4.343750"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_rel_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG scores the highest in terms of relevance! Note that, due to time limit, we only evaluate RAG's first response here. RAG also allows users to ask follow-up questions and it handles negations well if the user emphasizes the negation in the follow-up (e.g. \"Actually, I don't want floral notes.\"). RAG's revised reponses can potentially scores even higher than what we got above.   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wine-kernel",
   "language": "python",
   "name": "wine-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
